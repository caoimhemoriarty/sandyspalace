<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-02-22T13:25:36+00:00</updated><id>http://localhost:4000/atom.xml</id><title type="html">Sandyâ€™s Palace</title><subtitle>Rosa&apos;s gay little website</subtitle><entry><title type="html">New New Dedicated Server Time!</title><link href="http://localhost:4000/2025/02/22/New-New-Dedicated-Server-Time!.html" rel="alternate" type="text/html" title="New New Dedicated Server Time!" /><published>2025-02-22T00:00:00+00:00</published><updated>2025-02-22T00:00:00+00:00</updated><id>http://localhost:4000/2025/02/22/New-New-Dedicated-Server-Time!</id><content type="html" xml:base="http://localhost:4000/2025/02/22/New-New-Dedicated-Server-Time!.html"><![CDATA[<div>
    <p>
        The more I try to scale my servers, the more issues I hit:
        <ul>
            <li>Leaseweb has poor peering, and Hetzner is hostile to Plex media servers and public torrent trackers</li>
            <li>I would rather move away from Plex and support an open-source alternative</li>
            <li>Storage is a continual issue. To achieve 10TB+ cloud storage requires increasingly high server costs, and the only affordable options are the ones with issues listed above</li>
        </ul>
    </p>
    <p>
        My latest foray is attempting to alleviate the storage issue, such that the total HDD capacity of the server is capped at 8TB.
    </p>
    <p> 
        Currently the megatorrents take up 6.6TB, and I'm forecasting an increase of 1.2TB at the end of 2025. Some of these megatorrents are secondary, and don't require gigabit-line seeding support.
    </p>
    <p>
        As a secondary concern, the media server is currently 3.5TB in size. This could be alleviated by moving it to another network, or simply by scaling down the size of the library. Or by not downloading 4K but I'm not doing that :3
    </p>
    <p>
        Plus... price. I could keep paying higher and higher amounts for absurd storage and incredible connections, but we've passed the point where any of that benefits me. My charity work to the motorsports community has its limits.
    </p>
    <p>
        With this in mind, I'm on the hunt for a 6-8TB dedicated server, ideally with a gigabit line but I'd settle for 500Mpbs. To achieve this, some compromises need to be made:
        <ul>
            <li>The secondary-priority megatorrents need to be moved off the main server, maybe a week after they've been released to the general public</li>
            <li>The media server does need some degree of hosting on the server, as most of the media is obtained via private trackers. However I need to set up an automatic script that, once an item has hit its seeding ratio, it gets deleted from the server</li>
            <li>To achieve constrant streaming capabilities, I need a local media server rather than a remote one. The local server has a copy of all downloaded files, so once they're removed from the main server, there's no impact on streaming</li>
        </ul>
    </p>
    <p>
        There's one huge roadblock to the shuffling around of megatorrents and media files to local storage - local broadband. Even with a "gigabit" line, it's still residential, and therefore has dogshit upload speeds capped at 5MB/s.
    </p>
    <p>
        An opportunity arises from the fact that all of the primary megatorrents are updated in a narrow timeframe each year. Ideally all in the month of December. Additional space is required at this time, to allow me to host the old+new megatorrent for contingency reasons. But I could very feasibly rent a second server just for one month to alleviate this.
    </p>
    <p>
        Enter my radical and brilliant and flawless new plan that will definitely last longer than a week:
        <ul>
            <li>Use my PC rig as the media server, running Jellyfin ideally, but Emby or Plex as a backup</li>
            <li>Rent a nice compact 8TB server for megatorrent hosting, plus media downloading + seeding</li>
            <li>Build a script to automate the handling of fully-seeded torrents</li> 
        </ul>
    </p>
    <p>
        This requires the PC to be always-on, but it was always-on already :3
    </p>
    &lt;/p&gt;
        Today I'm sussing out some OVH servers, deciding between 500Mbps and 1Gbps, pulling the trigger, transferring everything over (again) and will report back when everything explodes
    &lt;/p&gt;


</div>]]></content><author><name></name></author><summary type="html"><![CDATA[The more I try to scale my servers, the more issues I hit: Leaseweb has poor peering, and Hetzner is hostile to Plex media servers and public torrent trackers I would rather move away from Plex and support an open-source alternative Storage is a continual issue. To achieve 10TB+ cloud storage requires increasingly high server costs, and the only affordable options are the ones with issues listed above My latest foray is attempting to alleviate the storage issue, such that the total HDD capacity of the server is capped at 8TB. Currently the megatorrents take up 6.6TB, and I'm forecasting an increase of 1.2TB at the end of 2025. Some of these megatorrents are secondary, and don't require gigabit-line seeding support. As a secondary concern, the media server is currently 3.5TB in size. This could be alleviated by moving it to another network, or simply by scaling down the size of the library. Or by not downloading 4K but I'm not doing that :3 Plus... price. I could keep paying higher and higher amounts for absurd storage and incredible connections, but we've passed the point where any of that benefits me. My charity work to the motorsports community has its limits. With this in mind, I'm on the hunt for a 6-8TB dedicated server, ideally with a gigabit line but I'd settle for 500Mpbs. To achieve this, some compromises need to be made: The secondary-priority megatorrents need to be moved off the main server, maybe a week after they've been released to the general public The media server does need some degree of hosting on the server, as most of the media is obtained via private trackers. However I need to set up an automatic script that, once an item has hit its seeding ratio, it gets deleted from the server To achieve constrant streaming capabilities, I need a local media server rather than a remote one. The local server has a copy of all downloaded files, so once they're removed from the main server, there's no impact on streaming There's one huge roadblock to the shuffling around of megatorrents and media files to local storage - local broadband. Even with a "gigabit" line, it's still residential, and therefore has dogshit upload speeds capped at 5MB/s. An opportunity arises from the fact that all of the primary megatorrents are updated in a narrow timeframe each year. Ideally all in the month of December. Additional space is required at this time, to allow me to host the old+new megatorrent for contingency reasons. But I could very feasibly rent a second server just for one month to alleviate this. Enter my radical and brilliant and flawless new plan that will definitely last longer than a week: Use my PC rig as the media server, running Jellyfin ideally, but Emby or Plex as a backup Rent a nice compact 8TB server for megatorrent hosting, plus media downloading + seeding Build a script to automate the handling of fully-seeded torrents This requires the PC to be always-on, but it was always-on already :3 &lt;/p&gt; Today I'm sussing out some OVH servers, deciding between 500Mbps and 1Gbps, pulling the trigger, transferring everything over (again) and will report back when everything explodes &lt;/p&gt;]]></summary></entry><entry><title type="html">New Dedicated Server Time!</title><link href="http://localhost:4000/2025/02/16/New-Dedicated-Server-Time!.html" rel="alternate" type="text/html" title="New Dedicated Server Time!" /><published>2025-02-16T00:00:00+00:00</published><updated>2025-02-16T00:00:00+00:00</updated><id>http://localhost:4000/2025/02/16/New-Dedicated-Server-Time!</id><content type="html" xml:base="http://localhost:4000/2025/02/16/New-Dedicated-Server-Time!.html"><![CDATA[<div>
    <p>
        Did I need a new server? Absolutely not. So here we are.
    </p>
    <p>
        My existing Leaseweb server (provided via Hosting By Design) is a fairly capable machine:
        <ul>
            <li>CPU: Xeon E3-1230</li>
            <li>RAM: 32GB</li>
            <li>Storage: 4 x 8 TB HDDs, configured in a RAID 5 array</li>
            <li>Connection: Dedicated 1GBps line, 100TB monthly traffic cap</li>
        </ul>
    </p>
    <p>
        However I want to make a few improvements.
    </p>
    <p>
        First, and most importantly, I'm fucking itchy and just love trying new shit out.
    </p>
    <p>
        Otherwise, I was eager to try different RAID configurations to see what could speed up my seedbox. Price wasn't a major concern, but I felt I wasn't utilising parts of my existing setup, mostly with the HDD configuration.
    </p>
    <p>
        Hetzner and OVH were my main alternative choices, however I couldn't justify the cost of the OVH servers. They were too over-specced on non-HDD elements, which I don't need as much right now. Therefore I waited for a decent price on a Hetzner server auction, and snapped this up:
        <ul>
            <li>CPU: Xeon E3-1271</li>
            <li>RAM: 32GB</li>
            <li>Storage: 4 x 10 TB HDDs, configured in a RAID 10 array</li>
            <li>Connection: Dedicated 1GBps line, unlimited traffic cap</li>
        </ul>
    </p>
    <p>
        Not exactly an earth-shattering change. The CPU upgrade isn't noteworthy. The monthly traffic cap likely won't exceed 50TB with my existing setup. So the major gains came from the RAID 10 configuration, which I am still exploring the optimisation side of.
    </p>
    <p>
        Here have been my adventures so far in getting it set up and replacing my old server:
    </p>
    <p>
        <b>Rescue system + Linux install:</b> Easy af, same as Leaseweb. RAID configuration was configured via the Hetzner rescue system rather than during setup with Hosting By Design, which was a neat change.
    </p>
    <p>
        <b>Swizzin install:</b> Same setup, just run 
<figure class="highlight"><pre><code class="language-plaintext" data-lang="plaintext">bash &lt;(curl -sL s5n.sh) &amp;&amp; . ~/.bashrc</code></pre></figure>
then stare at the Advanced Options while touching none of them, wait a few mins, and done.
    </p>
    <p>
        <b>Application install:</b> A bit trickier this time. All installs were done through Swizzin's Box system, same as last time - Nginx, File Browser, Jellyfin, Panel, QBitTorrent, Plex, Vsftpd. However, a huge roadblock was <a href="https://linustechtips.com/topic/1531520-plex-blocks-ip-addresses-from-hetzner-as-of-october-12-2023/"> Hetzner server IP addresses being blocked by Plex</a>. I tried working around this, but couldn't find a usable solution.
    </p>
    <p>
        Therefore I had to find a new media server, and Jellyfin (my preferred client) was notoriously difficult working with IP addresses with self-signed SSL certificates. This is fine when connecting via PCs and other tech you actually control, much less so on a Smart TV. They simply don't let the user override the security warning. The main solution to this seems to be buying a cheap domain name annually, getting an independent SSL certificate issued for it, then connecting your media server to it. However, I'm a lazy cheapskate :3
    </p>
    <p>
        Enter Emby, the semi-closed-source client that Jellyfin forked away from. Same as Plex, the Quick Connect feature is what let me circumvent the SSL self-signed certificate issue. Functionally it's 95% similar to Jellyfin, but I'm not as smug when using it. However, a huge advantage is that it can be located easily on Smart TV's app stores, making it much more straightforward to install versus Jellyfin, which required root/dev access.
    </p>
    <p>
        <b>Porting old server files</b>: Still ongoing. Most files are permanently seeded on QBitTorrent, so I just mass copied the Magnet IDs of the torrents to re-download them on the new server. Which has the additional bonus of letting the other seeds in the network share the network load :3
    </p>
    <p>
        For any files I wasn't torrenting, running 
<figure class="highlight"><pre><code class="language-plaintext" data-lang="plaintext">scp -r /directory_to_copy root@XXX.XXX.XXX.XX:/directory_to_copy_to </code></pre></figure>
 is sufficient. Probably. Still ongoing.
    </p>
    <p>
        Once fully ported, I plan to run some speed comparisons, test the performance gain of the RAID 10 configuration, then drop the slower server.
    </p>
    <p>
        <b>20th Feb Update:</b> I have decided that Hetzner actually sucks
    </p>
    <p>
        <img src="/assets/images/blog/hetzner.png" />
    </p>

</div>]]></content><author><name></name></author><summary type="html"><![CDATA[Did I need a new server? Absolutely not. So here we are. My existing Leaseweb server (provided via Hosting By Design) is a fairly capable machine: CPU: Xeon E3-1230 RAM: 32GB Storage: 4 x 8 TB HDDs, configured in a RAID 5 array Connection: Dedicated 1GBps line, 100TB monthly traffic cap However I want to make a few improvements. First, and most importantly, I'm fucking itchy and just love trying new shit out. Otherwise, I was eager to try different RAID configurations to see what could speed up my seedbox. Price wasn't a major concern, but I felt I wasn't utilising parts of my existing setup, mostly with the HDD configuration. Hetzner and OVH were my main alternative choices, however I couldn't justify the cost of the OVH servers. They were too over-specced on non-HDD elements, which I don't need as much right now. Therefore I waited for a decent price on a Hetzner server auction, and snapped this up: CPU: Xeon E3-1271 RAM: 32GB Storage: 4 x 10 TB HDDs, configured in a RAID 10 array Connection: Dedicated 1GBps line, unlimited traffic cap Not exactly an earth-shattering change. The CPU upgrade isn't noteworthy. The monthly traffic cap likely won't exceed 50TB with my existing setup. So the major gains came from the RAID 10 configuration, which I am still exploring the optimisation side of. Here have been my adventures so far in getting it set up and replacing my old server: Rescue system + Linux install: Easy af, same as Leaseweb. RAID configuration was configured via the Hetzner rescue system rather than during setup with Hosting By Design, which was a neat change. Swizzin install: Same setup, just run bash &lt;(curl -sL s5n.sh) &amp;&amp; . ~/.bashrc then stare at the Advanced Options while touching none of them, wait a few mins, and done. Application install: A bit trickier this time. All installs were done through Swizzin's Box system, same as last time - Nginx, File Browser, Jellyfin, Panel, QBitTorrent, Plex, Vsftpd. However, a huge roadblock was Hetzner server IP addresses being blocked by Plex. I tried working around this, but couldn't find a usable solution. Therefore I had to find a new media server, and Jellyfin (my preferred client) was notoriously difficult working with IP addresses with self-signed SSL certificates. This is fine when connecting via PCs and other tech you actually control, much less so on a Smart TV. They simply don't let the user override the security warning. The main solution to this seems to be buying a cheap domain name annually, getting an independent SSL certificate issued for it, then connecting your media server to it. However, I'm a lazy cheapskate :3 Enter Emby, the semi-closed-source client that Jellyfin forked away from. Same as Plex, the Quick Connect feature is what let me circumvent the SSL self-signed certificate issue. Functionally it's 95% similar to Jellyfin, but I'm not as smug when using it. However, a huge advantage is that it can be located easily on Smart TV's app stores, making it much more straightforward to install versus Jellyfin, which required root/dev access. Porting old server files: Still ongoing. Most files are permanently seeded on QBitTorrent, so I just mass copied the Magnet IDs of the torrents to re-download them on the new server. Which has the additional bonus of letting the other seeds in the network share the network load :3 For any files I wasn't torrenting, running scp -r /directory_to_copy root@XXX.XXX.XXX.XX:/directory_to_copy_to is sufficient. Probably. Still ongoing. Once fully ported, I plan to run some speed comparisons, test the performance gain of the RAID 10 configuration, then drop the slower server. 20th Feb Update: I have decided that Hetzner actually sucks]]></summary></entry><entry><title type="html">Me 0.1 Seconds After I Get A New Phone</title><link href="http://localhost:4000/2025/02/10/Me-0.1-Seconds-After-I-Get-A-New-Phone.html" rel="alternate" type="text/html" title="Me 0.1 Seconds After I Get A New Phone" /><published>2025-02-10T00:00:00+00:00</published><updated>2025-02-10T00:00:00+00:00</updated><id>http://localhost:4000/2025/02/10/Me-0.1-Seconds-After-I-Get-A-New-Phone</id><content type="html" xml:base="http://localhost:4000/2025/02/10/Me-0.1-Seconds-After-I-Get-A-New-Phone.html"><![CDATA[<p><img src="/assets/images/blog/graphene-1.jpg" alt="A picture of GrapheneOS being installed on a Pixel 8." /></p>

<div>
    <p>
        Yeah we running Graphene in this house keep scrolling
    </p>
</div>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">PiHole Blocking for Samsung Junk TVs</title><link href="http://localhost:4000/2025/02/05/PiHole-Blocking-for-Samsung-Junk-TVs.html" rel="alternate" type="text/html" title="PiHole Blocking for Samsung Junk TVs" /><published>2025-02-05T00:00:00+00:00</published><updated>2025-02-05T00:00:00+00:00</updated><id>http://localhost:4000/2025/02/05/PiHole-Blocking-for-Samsung-Junk-TVs</id><content type="html" xml:base="http://localhost:4000/2025/02/05/PiHole-Blocking-for-Samsung-Junk-TVs.html"><![CDATA[<div>
    <p>
        I hate modern technology, I fucking hate smart applications and the Internet of Things, and I utterly despite Smart TVs. Naturally I had to turn this into a hobby.
    </p>
    <p>
        I already have a Raspberry Pi 4B running <a href="https://pi-hole.net/">PiHole</a> for my home network, which I might make another blog post about. What this post relates to is the specific blocking of Samsung TV's dumb shit, and preventing it from updating the firmware.
    </p>
    <p>
        By default, Samsung Smart TVs send way too much analytics back to home base. Out of principle, I wanted to shut this down. Previous blocklists running on the PiHole proved this could work. 
    </p>
    <p>
        Also blocking firmware updates has now become crucial, as the newer updates include <a href="https://www.cnet.com/tech/home-entertainment/samsungs-2025-tvs-get-all-the-ai-extras-nobody-asked-for/">AI-generated total junk</a> that is burned into your home screen. The home screen is already fucking terrible as-is.
    </p>
    <p>
        With some light testing (read: adding lines until the updates stop and the TV doesn't break), I've created this blocklist that aims to block future updates, while still allowing the TV to function:
    </p>
    <p>
    <a href="https://sandyspalace.ie/assets/files/samsung_pihole.txt">Blocklist</a>
    </p>
    <p>
        I will update this post based on observations over the next few days. Feel free to link directly to the above URL, I've got plenty of server capacity.
    </p>
    <p>
        <b>5th Feb 2025</b>: PiHole is successfully blocking most Samsung CDN domains. Pop-up for firmware update hasn't appeared after a few soft resets. This may only occur a set amount of times per day, so I'll check for this tomorrow. TV otherwise working "well".
    </p>
    <p>
        <b>9th Feb 2025</b>: Popup notification for firmware updates have appeared again. I can't immediately see any queries going to Samsung domains. Fuckin delightful.
    </p>
    <p>
        <b>13th Feb 2025</b>: Still not seeing what request is slipping through PiHole. If the notification has been locally 'installed', then we could be in trouble. I'm never gonna install it, and can't figure out how to get it to fuck off.
    </p>
</div>]]></content><author><name></name></author><summary type="html"><![CDATA[I hate modern technology, I fucking hate smart applications and the Internet of Things, and I utterly despite Smart TVs. Naturally I had to turn this into a hobby. I already have a Raspberry Pi 4B running PiHole for my home network, which I might make another blog post about. What this post relates to is the specific blocking of Samsung TV's dumb shit, and preventing it from updating the firmware. By default, Samsung Smart TVs send way too much analytics back to home base. Out of principle, I wanted to shut this down. Previous blocklists running on the PiHole proved this could work. Also blocking firmware updates has now become crucial, as the newer updates include AI-generated total junk that is burned into your home screen. The home screen is already fucking terrible as-is. With some light testing (read: adding lines until the updates stop and the TV doesn't break), I've created this blocklist that aims to block future updates, while still allowing the TV to function: Blocklist I will update this post based on observations over the next few days. Feel free to link directly to the above URL, I've got plenty of server capacity. 5th Feb 2025: PiHole is successfully blocking most Samsung CDN domains. Pop-up for firmware update hasn't appeared after a few soft resets. This may only occur a set amount of times per day, so I'll check for this tomorrow. TV otherwise working "well". 9th Feb 2025: Popup notification for firmware updates have appeared again. I can't immediately see any queries going to Samsung domains. Fuckin delightful. 13th Feb 2025: Still not seeing what request is slipping through PiHole. If the notification has been locally 'installed', then we could be in trouble. I'm never gonna install it, and can't figure out how to get it to fuck off.]]></summary></entry><entry><title type="html">Celeste Is Good</title><link href="http://localhost:4000/2025/01/30/Celeste-Is-Good.html" rel="alternate" type="text/html" title="Celeste Is Good" /><published>2025-01-30T00:00:00+00:00</published><updated>2025-01-30T00:00:00+00:00</updated><id>http://localhost:4000/2025/01/30/Celeste-Is-Good</id><content type="html" xml:base="http://localhost:4000/2025/01/30/Celeste-Is-Good.html"><![CDATA[<div>
    <p>
        Ticking off another game on the "quintessential games for enlightened catgirls" list, I played Celeste this month for the first time.
    </p>
    <p>
        It was my first proper 2D platformer since... one of the older Castlevania games I think? And it was an incredible title to pick - the level design, mechanics, and soundtrack are all built with a wonderful amount of love.
    </p>
    <p>
        I'm completely hooked, and am now going back to pick up every strawberry, and make a solid attempt at the B-sides. Chapter 8 and 9 will soon follow :3
    </p>
</div>

<p><img src="/assets/images/blog/celeste1.png" alt="A screenshot of Celeste after the first playthough. 94 out of 175 strawberries, 1738 deaths, and 9 hours and 54 minutes playtime." /></p>

<div>
    <p>
    In my defense a few hundred of these deaths were early B-side attempts lolololol
    </p>
</div>]]></content><author><name></name></author><summary type="html"><![CDATA[Ticking off another game on the "quintessential games for enlightened catgirls" list, I played Celeste this month for the first time. It was my first proper 2D platformer since... one of the older Castlevania games I think? And it was an incredible title to pick - the level design, mechanics, and soundtrack are all built with a wonderful amount of love. I'm completely hooked, and am now going back to pick up every strawberry, and make a solid attempt at the B-sides. Chapter 8 and 9 will soon follow :3]]></summary></entry><entry><title type="html">I Love Niche Problems</title><link href="http://localhost:4000/2025/01/07/I-Love-Niche-Problems.html" rel="alternate" type="text/html" title="I Love Niche Problems" /><published>2025-01-07T00:00:00+00:00</published><updated>2025-01-07T00:00:00+00:00</updated><id>http://localhost:4000/2025/01/07/I-Love-Niche-Problems</id><content type="html" xml:base="http://localhost:4000/2025/01/07/I-Love-Niche-Problems.html"><![CDATA[<div>
    <p>
        At this point I still haven't typed up an explanation of my F1 archiving and distribution work, so out of context this will appear even more insane than usual.
    </p>
    <p>
        Long story short, I've been building up a rather popular niche in the high seas community. My archive of F1 media is solid, comparable to plenty of other buffs around the world, but I've made a commitment to sharing it as widely as possible, for as long as possible.
    </p>
    <p>
        It's more of a long-term safe storage solution, as most casual internet users won't go beyond a simple stream. But for those willing to make the extra effort, I've been making some great progress on building up the greatest archive of F1 media imagineable, one totally impossible within the confines of archaic copyright law.
    </p>
    <p>
        There's still years to go. But I receive the most wonderful messages from people online. Some talk about reliving their childhood memories watching their first ever season, decades on. Others have found special moments they never even knew existed in the sport. And some just love the archiving work.
    </p>
    <p>
        These are my toughest customers, and also my favourites. Because at no point before in my life did I ever think I'd receive a message on Reddit from a stranger, offering me the inaugural race of the 1985 Formula 3000 season with such eagerness. And I was always going to say yes please and thank you.
    </p>
    <p>
        The fellow crackheads continue their relentless assault. Three different collectors have come forward - two for F1, one for MotoGP. We will argue over broadcaster feeds, commentary tracks, video encoding standards. Together we will comb and merge everything, and create a work even more comprehensive than last year. And then we'll do it again, and again, and again.
    </p>
    <p>
        Such wonderful situations pose the most wonderful problems. Such as - how the fuck can I keep effectively storing this stuff? And how the hell am I going to have all the time to work through this?
    </p>
    <p>
        FOM wishes they had our freedom.
    </p>
</div>]]></content><author><name></name></author><summary type="html"><![CDATA[At this point I still haven't typed up an explanation of my F1 archiving and distribution work, so out of context this will appear even more insane than usual. Long story short, I've been building up a rather popular niche in the high seas community. My archive of F1 media is solid, comparable to plenty of other buffs around the world, but I've made a commitment to sharing it as widely as possible, for as long as possible. It's more of a long-term safe storage solution, as most casual internet users won't go beyond a simple stream. But for those willing to make the extra effort, I've been making some great progress on building up the greatest archive of F1 media imagineable, one totally impossible within the confines of archaic copyright law. There's still years to go. But I receive the most wonderful messages from people online. Some talk about reliving their childhood memories watching their first ever season, decades on. Others have found special moments they never even knew existed in the sport. And some just love the archiving work. These are my toughest customers, and also my favourites. Because at no point before in my life did I ever think I'd receive a message on Reddit from a stranger, offering me the inaugural race of the 1985 Formula 3000 season with such eagerness. And I was always going to say yes please and thank you. The fellow crackheads continue their relentless assault. Three different collectors have come forward - two for F1, one for MotoGP. We will argue over broadcaster feeds, commentary tracks, video encoding standards. Together we will comb and merge everything, and create a work even more comprehensive than last year. And then we'll do it again, and again, and again. Such wonderful situations pose the most wonderful problems. Such as - how the fuck can I keep effectively storing this stuff? And how the hell am I going to have all the time to work through this? FOM wishes they had our freedom.]]></summary></entry><entry><title type="html">Hello World and FUCK JEKYLL</title><link href="http://localhost:4000/2024/12/22/Hello-World-and-FUCK-JEKYLL.html" rel="alternate" type="text/html" title="Hello World and FUCK JEKYLL" /><published>2024-12-22T00:00:00+00:00</published><updated>2024-12-22T00:00:00+00:00</updated><id>http://localhost:4000/2024/12/22/Hello-World-and-FUCK-JEKYLL</id><content type="html" xml:base="http://localhost:4000/2024/12/22/Hello-World-and-FUCK-JEKYLL.html"><![CDATA[<div>
    <p>
        The initial opening line here was complaining at how the all-caps didn't render properly on the home page, until it just did. I'm not ignoring this.
    </p>
    <p>
        This supposedly simple project has been rendered excruciatingly difficult at every single stage by this utter piece of shit. In no particular order:
        <ul>
            <li>Documentation is totally at odds with GitHub Pages tutorials, despite Jekyll being the recommended site builder tool</li>
            <li>Directories and files that should have been generated with 
<figure class="highlight"><pre><code class="language-plaintext" data-lang="plaintext">jekyll new my-awesome-site</code></pre></figure>
just... weren't. Their own documentation then expects you to be working with files that simply don't exist, and make no mention of how/where to create them</li>
            <li>Most themes built for Jekyll are not supported by GitHub Pages, nobody mentions this until far too late</li>
            <li>Compatability between Ruby's gems and Bundler is appalling. I have two different versions of so many things, which is gonna cause huge problems down the road</li>
            <li>Whoever wrote the documentation for Liquid needs to be beaten to death, as an act of sympathy and mercy</li>
        </ul>
    </p>
    <p>
        However we are here, with a functioning page on GitHub Pages, mostly because I gave up on hosting it on a dedicated server that also hosted a Swizzin seedbox. Far too many Nginx overrides meant I couldn't get the domain name to point to Sandy's Palace properly, so I've split the two, and left the seedbox to fester on its own IP address.
    </p>
    <p>
        Despite all forces in the universe trying to make it otherwise, I am committed to this being a fun and gay little website. Until I meet the Jekyll devs.
    </p>
</div>]]></content><author><name></name></author><summary type="html"><![CDATA[The initial opening line here was complaining at how the all-caps didn't render properly on the home page, until it just did. I'm not ignoring this. This supposedly simple project has been rendered excruciatingly difficult at every single stage by this utter piece of shit. In no particular order: Documentation is totally at odds with GitHub Pages tutorials, despite Jekyll being the recommended site builder tool Directories and files that should have been generated with jekyll new my-awesome-site just... weren't. Their own documentation then expects you to be working with files that simply don't exist, and make no mention of how/where to create them Most themes built for Jekyll are not supported by GitHub Pages, nobody mentions this until far too late Compatability between Ruby's gems and Bundler is appalling. I have two different versions of so many things, which is gonna cause huge problems down the road Whoever wrote the documentation for Liquid needs to be beaten to death, as an act of sympathy and mercy However we are here, with a functioning page on GitHub Pages, mostly because I gave up on hosting it on a dedicated server that also hosted a Swizzin seedbox. Far too many Nginx overrides meant I couldn't get the domain name to point to Sandy's Palace properly, so I've split the two, and left the seedbox to fester on its own IP address. Despite all forces in the universe trying to make it otherwise, I am committed to this being a fun and gay little website. Until I meet the Jekyll devs.]]></summary></entry><entry><title type="html">To Do List for this Webbed Site</title><link href="http://localhost:4000/2024/10/10/to-do-list-for-this-webbed-site.html" rel="alternate" type="text/html" title="To Do List for this Webbed Site" /><published>2024-10-10T00:00:00+01:00</published><updated>2024-10-10T00:00:00+01:00</updated><id>http://localhost:4000/2024/10/10/to-do-list-for-this-webbed-site</id><content type="html" xml:base="http://localhost:4000/2024/10/10/to-do-list-for-this-webbed-site.html"><![CDATA[<div>
  <p>
    At time of writing, this site has a basic home page, navbar, Blog Archive, and About page all typed up.
  </p>
  <p>
    It's been created via <a href="https://jekyllrb.com/">Jekyll</a> as a static site generator, and the theme is a lightly modified version of <a href="http://jekyllthemes.org/themes/dark-poole/">Dark Poole</a>.
  </p>
  <p>
    Hosting is done by my own dedicated server, which also hosts a few other fun purposes. TLS certificates are done by <a href="https://letsencrypt.org/">Let's Encrypt</a>, and domain management by <a href="https://www.blacknight.com/">Blacknight</a>.
  </p>
  <p>
    While the site is in its simplest possible form, and before I start overcomplicating things, there's a few improvements to be made:
      <ul>
        <li>Get an 88x31 button created<s>, and new favicon icon</s></li>
        <li>Expand the home page to have a few more links to projects</li>
        <li>Create a basic webring</li>
        <li><s>Create a deploy script to automate the actions of Jekyll building, testing nginx, then reloading nginx via systemctl</s></li>
        <li><s>Build an easy and reliable way of editing/creating from my various machines. Currently I'm using Putty to SSH into the server, then using Nano to create and edit files... it's not fantastic</s>Put VS Code onto my main machine, it's alright, Putty does the job for the simple work, couldn't be arsed doing anything grander than that</li>
        <li><s>Link in the subdomain for the seedbox, for no reason other than showing off</s></li>
        <li><s>Add an icon of a little cat paw when hovering over the navbar :3</s> Already close to giving up on this one, it's a pain in the ass that I don't fully understand yet</li>
      </ul>
  </p>
</div>]]></content><author><name></name></author><summary type="html"><![CDATA[At time of writing, this site has a basic home page, navbar, Blog Archive, and About page all typed up. It's been created via Jekyll as a static site generator, and the theme is a lightly modified version of Dark Poole. Hosting is done by my own dedicated server, which also hosts a few other fun purposes. TLS certificates are done by Let's Encrypt, and domain management by Blacknight. While the site is in its simplest possible form, and before I start overcomplicating things, there's a few improvements to be made: Get an 88x31 button created, and new favicon icon Expand the home page to have a few more links to projects Create a basic webring Create a deploy script to automate the actions of Jekyll building, testing nginx, then reloading nginx via systemctl Build an easy and reliable way of editing/creating from my various machines. Currently I'm using Putty to SSH into the server, then using Nano to create and edit files... it's not fantasticPut VS Code onto my main machine, it's alright, Putty does the job for the simple work, couldn't be arsed doing anything grander than that Link in the subdomain for the seedbox, for no reason other than showing off Add an icon of a little cat paw when hovering over the navbar :3 Already close to giving up on this one, it's a pain in the ass that I don't fully understand yet]]></summary></entry></feed>